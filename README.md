# *RoboTransfer*: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer
[![ğŸŒ Project Page](https://img.shields.io/badge/ğŸŒ-Project_Page-blue)](https://horizonrobotics.github.io/robot_lab/robotransfer)
[![ğŸ“„ arXiv](https://img.shields.io/badge/ğŸ“„-arXiv-b31b1b)](https://arxiv.org/abs/2505.23171)
[![ğŸ¥ Video](https://img.shields.io/badge/ğŸ¥-Video-red)](https://youtu.be/dGXKtqDnm5Q)
[![ä¸­æ–‡ä»‹ç»](https://img.shields.io/badge/ä¸­æ–‡ä»‹ç»-07C160?logo=wechat&logoColor=white)](https://mp.weixin.qq.com/s/c9-1HPBMHIy4oEwyKnsT7Q)
[![æœºå™¨ä¹‹å¿ƒä»‹ç»](https://img.shields.io/badge/æœºå™¨ä¹‹å¿ƒä»‹ç»-07C160?logo=wechat&logoColor=white)](https://mp.weixin.qq.com/s/Hj2h3nxO8XxPeqd3OhctKA)

> ***RoboTransfer***, a diffusion-based video generation framework for robotic data synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry with explicit control over scene components, such as background and object attributes. By incorporating cross-view feature interactions and global depth/normal conditions, RoboTransfer ensures geometry consistency across views. This framework allows fine-grained control, including background edits and object swaps.
<img src="/assets/pin/robotransfer.jpg" alt="Overall Framework" width="700"/>

## âœ… Setup Environment

We use uv to manage dependencies, to get our environments:
```bash
git clone https://github.com/HorizonRobotics/RoboTransfer.git
cd RoboTransfer
export UV_HTTP_TIMEOUT=600
uv sync
uv pip install -e .
```

## ğŸš€ Inference

```bash
uv run main.py # --mem_efficient for 4090
```


## ğŸ“ˆ More Inference Data

Update the dependencies of the data pipeline.
```bash
uv sync --extra data
```

### âš™ï¸ For more sim data

You can obtain more simulation data from the [RoboTwin CVPR Challenge](https://github.com/RoboTwin-Platform/RoboTwin/tree/CVPR-Challenge-2025-Round1).
 <!-- Alternatively, you can use the collected data in [RoboTwin2.0-aloha-agilex](https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/dataset/aloha-agilex). -->
You can then use the process_sim.sh script to convert raw data (.pickle files and .hdf5) into the RoboTransfer format with geometric conditioning.

```bash
script/process_sim.sh
```

### ğŸ¤– For more real data
For real-world data collected by the ALOHA-AgileX robot system, access the dataset [RoboTransfer-RealData](https://huggingface.co/datasets/HorizonRobotics/RoboTransfer-RealData). You can then process raw RGB images using the process_real.sh script to convert them into RoboTransfer format with geometric conditioning.

```bash
script/process_real.sh
```


## ğŸ™Œ Acknowledgement

RoboTransfer builds upon the following amazing projects and models:
ğŸŒŸ [Video-Depth-Anything](https://github.com/DepthAnything/Video-Depth-Anything)
ğŸŒŸ [Lotus](https://github.com/EnVision-Research/Lotus)
ğŸŒŸ [GPT4o](https://platform.openai.com/docs/models/gpt-4o)
ğŸŒŸ [GroundSam](https://github.com/IDEA-Research/Grounded-Segment-Anything)
ğŸŒŸ [IOPaint](https://github.com/Sanster/IOPaint)

##  âš–ï¸ License
This project is licensed under the [Apache License 2.0](LICENSE). See the `LICENSE` file for details.

## ğŸ“š Citation
If you use RoboTransfer in your research or projects, please cite:

```bibtex
@misc{liu2025robotransfergeometryconsistentvideodiffusion,
      title={RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer},
      author={Liu Liu and Xiaofeng Wang and Guosheng Zhao and Keyu Li and Wenkang Qin and Jiaxiong Qiu and Zheng Zhu and Guan Huang and Zhizhong Su},
      year={2025},
      eprint={2505.23171},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.23171},
}
```
